{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8222e63f-7e4a-4607-a88a-da5752a2f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Libraries #####\n",
    "import dotenv\n",
    "dotenv.load_dotenv(\".env\")\n",
    "import os\n",
    "import time\n",
    "import utils\n",
    "import shutil\n",
    "import argparse\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(0)\n",
    "tf.keras.utils.set_random_seed(0)\n",
    "import models_tensorflow.EEGModels\n",
    "from typing import Literal\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras import utils as tf_utils\n",
    "backend.set_image_data_format(\"channels_last\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import utils\n",
    "from utils import Metric, plot_confusion_matrix, plot_history\n",
    "from libs.dataset import BcicIv2aDataset, InnerSpeechDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a459cc27-5147-42a3-947f-e91a582d3e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DATASET_DIR\"] = 'datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eca5120-a92a-44e7-927d-8604417ec586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading BCIC IV 2a dataset - A09E: 100%|████████| 18/18 [00:22<00:00,  1.27s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset = BcicIv2aDataset()  # l_freq=4\n",
    "inputs, truths = dataset.all_data_and_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22090b3b-af30-44d5-87a7-eea2479d5c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5184, 22, 257)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "381b819f-9722-42cf-ac10-7151a9eb7e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5184, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1feb17e-e3e3-474d-9d90-0df4b5d68c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False,  True])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94054874-d99f-441b-b04b-e5e6aaa380ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5011255-66b8-4911-9e2b-76257a3e99a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge tensorflow-addons -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66f637ed-3efa-4acd-8df0-6e598f74edf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5184, 257, 22)\n"
     ]
    }
   ],
   "source": [
    "inputs = np.transpose(inputs, (0, 2, 1))\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e074e47-2914-4377-9cfe-40b730b262b9",
   "metadata": {},
   "source": [
    "## Transformer Build and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe1527d0-bd6f-403b-9cc9-cf8a16ba071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d681cd0-5fc5-4c62-97d7-2aa08e7be9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import MultiHeadAttention, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# Time2Vec layer: This layer provides a method to encode both linear and periodic components of time into the model inputs.\n",
    "class Time2Vec(keras.layers.Layer):\n",
    "    def __init__(self, kernel_size=1):\n",
    "        super(Time2Vec, self).__init__(trainable=True, name='Time2VecLayer')\n",
    "        self.k = kernel_size\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # trend\n",
    "        self.wb = self.add_weight(name='wb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n",
    "        self.bb = self.add_weight(name='bb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n",
    "        # periodic\n",
    "        self.wa = self.add_weight(name='wa',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n",
    "        self.ba = self.add_weight(name='ba',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n",
    "        super(Time2Vec, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, **kwargs):\n",
    "        bias = self.wb * inputs + self.bb\n",
    "        # dp = K.dot(inputs, self.wa) + self.ba\n",
    "        dp = tf.reduce_sum(inputs * self.wa, axis=-1, keepdims=True) + self.ba  # Weighted sum across the time dimension\n",
    "        # wgts = K.sin(dp) # or K.cos(.)\n",
    "        wgts = tf.math.sin(dp)  # Apply the sinusoidal function\n",
    "        \n",
    "\n",
    "        ret = K.concatenate([K.expand_dims(bias, -1), wgts], -1)\n",
    "        ret = K.reshape(ret, (-1, inputs.shape[1]*(self.k+1)))\n",
    "        \n",
    "        return ret\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1]*(self.k + 1))\n",
    "\n",
    "\n",
    "\n",
    "# AttentionBlock: This is a custom layer that incorporates multi-head self-attention mechanism, allowing the model to focus on different parts of the input sequence.\n",
    "# class AttentionBlock(keras.Model):\n",
    "#     def __init__(self, num_heads=2, head_size=128, ff_dim=256, dropout=0.1, feature_dim=515, **kwargs):\n",
    "#         super(AttentionBlock, self).__init__(**kwargs)\n",
    "#         self.num_heads = num_heads\n",
    "#         self.head_size = head_size\n",
    "#         self.dropout = dropout\n",
    "#         self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=head_size, dropout=dropout)\n",
    "#         self.att_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "#         self.ff_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "#         self.ff_conv1 = Dense(ff_dim, activation='relu')\n",
    "#         self.ff_conv2 = Dense(feature_dim)  # Ensure this matches the input feature dimension\n",
    "\n",
    "#     def call(self, inputs, training=False):\n",
    "#         attn_output = self.attention(query=inputs, key=inputs, value=inputs)\n",
    "#         attn_output = Dropout(self.dropout)(attn_output, training=training)\n",
    "#         out1 = self.att_norm(inputs + attn_output)\n",
    "\n",
    "#         ffn_output = self.ff_conv1(out1)\n",
    "#         ffn_output = self.ff_conv2(ffn_output)\n",
    "#         ffn_output = Dropout(self.dropout)(ffn_output, training=training)\n",
    "#         return self.ff_norm(out1 + ffn_output)\n",
    "\n",
    "class AttentionBlock(keras.Model):\n",
    "    def __init__(self, name='AttentionBlock', num_heads=2, head_size=128, ff_dim=None, dropout=0, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "\n",
    "        if ff_dim is None:\n",
    "            ff_dim = head_size\n",
    "\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=head_size, dropout=dropout)\n",
    "        self.attention_dropout = keras.layers.Dropout(dropout)\n",
    "        self.attention_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.ff_conv1 = keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')\n",
    "        # self.ff_conv2 at build()\n",
    "        self.ff_dropout = keras.layers.Dropout(dropout)\n",
    "        self.ff_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.ff_conv2 = keras.layers.Conv1D(filters=input_shape[-1], kernel_size=1) \n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.attention(query=inputs, key=inputs, value=inputs)\n",
    "        x = self.attention_dropout(x)\n",
    "        x = self.attention_norm(inputs + x)\n",
    "\n",
    "        x = self.ff_conv1(x)\n",
    "        x = self.ff_conv2(x)\n",
    "        x = self.ff_dropout(x)\n",
    "\n",
    "        x = self.ff_norm(inputs + x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# CAN NAME THIS EEGTransformerModel:\n",
    "# TransformerModel: This class defines the overall model architecture combining Time2Vec and multiple AttentionBlocks.\n",
    "class TransformerModel(keras.Model):\n",
    "    def __init__(self, time2vec_dim=1, num_heads=2, head_size=128, ff_dim=None, num_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.time2vec = Time2Vec(kernel_size=time2vec_dim)\n",
    "        self.attention_layers = [\n",
    "            AttentionBlock(num_heads=num_heads, head_size=head_size, ff_dim=head_size if ff_dim is None else ff_dim, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.global_pool = layers.GlobalAveragePooling1D()\n",
    "        self.output_layer = layers.Dense(22, activation='linear')  # Assuming regression type output for each channel\n",
    "\n",
    "    def call(self, inputs):\n",
    "        time_embeddings = layers.TimeDistributed(self.time2vec)(inputs)\n",
    "        x = tf.concat([inputs, time_embeddings], axis=-1)\n",
    "        for attention_layer in self.attention_layers:\n",
    "            x = attention_layer(x)\n",
    "        x = self.global_pool(x)\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8bc7757c-ba77-497d-8403-c8675cb70bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5184, 257, 22)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "932fe671-a16a-413c-a820-265aa58282ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example function to load dataset (this should be replaced with actual data loading)\n",
    "def load_dataset():\n",
    "    x =  inputs #  np.random.rand(100, 22, 257)  # Simulated input data\n",
    "    y =  inputs # np.random.rand(100, 22, 257)  # Simulated ground truth data\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e12c962-74f9-45d0-962e-9936e40c7b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, truths = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eee601f8-2d28-4a5a-a393-31dc81712d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5184, 257, 22)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb7400e3-ac7a-44dd-b926-9250881bb95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data to range [0, 1] for consistency and to aid learning.\n",
    "inputs = (inputs - np.min(inputs)) / (np.max(inputs) - np.min(inputs))\n",
    "truths = (truths - np.min(truths)) / (np.max(truths) - np.min(truths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ce5b0d3-c030-4d59-aebb-d83b3bd77111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets to evaluate model performance.\n",
    "x_train, x_test, y_train, y_test = train_test_split(inputs, truths, test_size=0.2, random_state=42)\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(scaled_inputs, truths, test_size=0.2, random_state=42)\n",
    "\n",
    "# history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d467d66-e765-43ee-9160-8afe1a0c164e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4147, 257, 22)\n",
      "(4147, 257, 22)\n",
      "(1037, 257, 22)\n",
      "(1037, 257, 22)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33f6fa89-854c-440f-a33b-c2ab8e861e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Configure and compile the model\n",
    "# model = TransformerModel(time2vec_dim=1, num_heads=2, head_size=128, ff_dim=256, num_layers=3, dropout=0.1)\n",
    "# model.compile(optimizer='adam', loss='mean_squared_error')  # Use Adam optimizer and MSE loss function\n",
    "\n",
    "# Model instantiation and compilation\n",
    "model = TransformerModel(time2vec_dim=1, num_heads=2, head_size=128, num_layers=1, dropout=0.1)\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fc420fe-ddfe-46cb-ad77-46a1ce081421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2a5d5c4-ffe1-4213-bbbe-5ef4a6c648c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Learning Rate Scheduler to adjust the learning rate dynamically during training for better performance.\n",
    "def lr_scheduler(epoch, lr):\n",
    "    warmup_epochs = 15\n",
    "    decay_epochs = 100\n",
    "    initial_lr = 1e-6\n",
    "    base_lr = 1e-3\n",
    "    min_lr = 5e-5\n",
    "    if epoch <= warmup_epochs:\n",
    "        pct = epoch / warmup_epochs\n",
    "        return ((base_lr - initial_lr) * pct) + initial_lr\n",
    "    if epoch > warmup_epochs and epoch < warmup_epochs + decay_epochs:\n",
    "        pct = 1 - ((epoch - warmup_epochs) / decay_epochs)\n",
    "        return ((base_lr - min_lr) * pct) + min_lr\n",
    "    return min_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f154da63-2e71-475e-b6de-3b9f4a951039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras import backend as K\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "932e2092-9ce4-49f4-bc82-bf912cbf7d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Exception encountered when calling TransformerModel.call().\n\n\u001b[1mtuple index out of range\u001b[0m\n\nArguments received by TransformerModel.call():\n  • inputs=tf.Tensor(shape=(None, 257, 22), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m callback_list \u001b[38;5;241m=\u001b[39m [LearningRateScheduler(lr_scheduler, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Train the model with the specified configurations.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(x_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(x_test, y_test), batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/eeg/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[30], line 134\u001b[0m, in \u001b[0;36mTransformerModel.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m--> 134\u001b[0m     time_embeddings \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mTimeDistributed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime2vec)(inputs)\n\u001b[1;32m    135\u001b[0m     x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat([inputs, time_embeddings], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attention_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_layers:\n",
      "Cell \u001b[0;32mIn[30], line 45\u001b[0m, in \u001b[0;36mTime2Vec.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_shape):\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_weight(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, shape\u001b[38;5;241m=\u001b[39m(input_shape[\u001b[38;5;241m2\u001b[39m],), initializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m'\u001b[39m, trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_weight(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbb\u001b[39m\u001b[38;5;124m'\u001b[39m, shape\u001b[38;5;241m=\u001b[39m(input_shape[\u001b[38;5;241m2\u001b[39m],), initializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m'\u001b[39m, trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwa \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_weight(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwa\u001b[39m\u001b[38;5;124m'\u001b[39m, shape\u001b[38;5;241m=\u001b[39m(input_shape[\u001b[38;5;241m2\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk), initializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m'\u001b[39m, trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mIndexError\u001b[0m: Exception encountered when calling TransformerModel.call().\n\n\u001b[1mtuple index out of range\u001b[0m\n\nArguments received by TransformerModel.call():\n  • inputs=tf.Tensor(shape=(None, 257, 22), dtype=float32)"
     ]
    }
   ],
   "source": [
    "callback_list = [LearningRateScheduler(lr_scheduler, verbose=1)]\n",
    "\n",
    "# Train the model with the specified configurations.\n",
    "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test), batch_size=32) # callbacks=callback_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "19486b1a-4923-4324-8d5d-46b719d5e911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step\n",
      "Shape of the predicted outputs: (4147, 22, 257)\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'model' is the trained Transformer model and 'x_test' is the test dataset\n",
    "\n",
    "# Predict the outputs for the test set\n",
    "predicted_outputs = model.predict(x_train)\n",
    "\n",
    "# You can now use 'predicted_outputs' for further analysis, visualization, or post-processing\n",
    "# For example, printing the shape of the outputs and some sample data\n",
    "print(\"Shape of the predicted outputs:\", predicted_outputs.shape)\n",
    "# print(\"Sample predictions:\", predicted_outputs[:5])\n",
    "\n",
    "# If you need to compare these predictions with the actual labels\n",
    "# Assuming 'y_test' contains the true values for the test set\n",
    "# print(\"Actual true outputs:\", y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4706829a-8bca-44d4-bbb1-bd2fe3c7ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # calculate the performance metrics, 'Mean Squared Error'\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# mse = mean_squared_error(y_train, predicted_outputs)\n",
    "# print(\"Mean Squared Error on Test Set:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "30b174d0-098c-407c-83e0-4b07b589c1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error per feature, averaged over time steps: 0.002535971542483811\n"
     ]
    }
   ],
   "source": [
    "# Calculate MSE for each feature and average over all time steps\n",
    "mse_per_feature = np.mean([mean_squared_error(y_train[:, i, :], predicted_outputs[:, i, :]) for i in range(y_train.shape[1])])\n",
    "print(\"Mean Squared Error per feature, averaged over time steps:\", mse_per_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1f595325-9238-4bdb-8d4d-7577eb5973fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Channel 1: 0.002892112152690684\n",
      "MSE for Channel 2: 0.002365045193164491\n",
      "MSE for Channel 3: 0.00265804448155546\n",
      "MSE for Channel 4: 0.002930883197617\n",
      "MSE for Channel 5: 0.0027586019778895925\n",
      "MSE for Channel 6: 0.0025873186748943417\n",
      "MSE for Channel 7: 0.001653848754940294\n",
      "MSE for Channel 8: 0.002205535325460935\n",
      "MSE for Channel 9: 0.0024582938756672764\n",
      "MSE for Channel 10: 0.002783532164773731\n",
      "MSE for Channel 11: 0.0026102892130813664\n",
      "MSE for Channel 12: 0.002585270767988162\n",
      "MSE for Channel 13: 0.002167161265563858\n",
      "MSE for Channel 14: 0.002296732904307401\n",
      "MSE for Channel 15: 0.0024126604748039232\n",
      "MSE for Channel 16: 0.002615999525986123\n",
      "MSE for Channel 17: 0.0025995530125059595\n",
      "MSE for Channel 18: 0.0027031042723586005\n",
      "MSE for Channel 19: 0.0024519940378041174\n",
      "MSE for Channel 20: 0.002726288487523727\n",
      "MSE for Channel 21: 0.0026341128879922068\n",
      "MSE for Channel 22: 0.0026949912860745858\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Initialize a list or numpy array to store the MSE values for each channel\n",
    "mse_per_channel = np.zeros((y_train.shape[1],))  # y_train.shape[1] should be 22 if there are 22 channels\n",
    "\n",
    "# Loop through each channel and calculate MSE\n",
    "for channel_index in range(y_train.shape[1]):\n",
    "    # Extract the channel data for both true and predicted values\n",
    "    true_channel_data = y_train[:, channel_index, :]\n",
    "    predicted_channel_data = predicted_outputs[:, channel_index, :]\n",
    "\n",
    "    # Compute the MSE for this channel\n",
    "    mse_per_channel[channel_index] = mean_squared_error(true_channel_data, predicted_channel_data)\n",
    "\n",
    "    # Optionally, print the MSE for each channel\n",
    "    print(f\"MSE for Channel {channel_index + 1}: {mse_per_channel[channel_index]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5cd24e-ddf9-4787-8fbe-c30e30c0c088",
   "metadata": {},
   "source": [
    "To DO:\n",
    "We plan to jointly train this Transformer Denoiser network with EEGNet using a joint Loss function: Reconstruction loss for Denoiser + BCE loss for classification. \n",
    "\n",
    "Training standalone Transformer on reconstruction task is not necessary unless to make sure it's working. Fine tuning model hyperparameters and architecture has to be performed jointly on Transformer(Denoiser) + EEGNet pipeline, with frozen weights for EEGNet. \n",
    "\n",
    "Immediate task: Train the EEGNet by running necessary scripts and save the model weights, as they are currently missing. \n",
    "\n",
    "Next, import the EEGNet model from python script as a library into the Transformer notebook, instantiate the model, load the weights, and make trainable=False to freeze the weights. \n",
    "\n",
    "Then, build the pipeline of Transformer + EEGNet and optimize(train) the Transformer model using the joint loss function. \n",
    "\n",
    "Finally, write this as a script.\n",
    "\n",
    "Note: For Transformer architecture, you can use classical time encoding methods or use learning methods like time2vec with trainable weights\n",
    "\n",
    "Note: You can either design the Denoiser as an offset producer (have a skip connection from input and subtract it with pre-final output to effectively output the offset requiret to Denoise EEG data. OR view the Denoiser as a signal enhancer that will produce a cleaner version of raw signal with only important features, removing irrelevant details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0cd924-c44c-4e8e-9cb0-6a335ae01ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af3ac03-3bf5-472e-8989-10d5f3902653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

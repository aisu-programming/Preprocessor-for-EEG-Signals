{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from detection_datasets import DetectionDataset\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import io\n",
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from datasets import Dataset\n",
    "import io"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the below code block to get train parquet files, and test parquet\n",
    "#### Adjust the iteration accordingly to the number of parquet files needed. \n",
    "##### For me, 4 parquet files for train and 1 parquet fiel for test worked seemlessly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [03:35<00:00, 417.02it/s]\n",
      "100%|██████████| 90000/90000 [03:37<00:00, 413.87it/s]\n",
      "100%|██████████| 90000/90000 [03:36<00:00, 416.40it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/Volumes/T7 Shield/Downloads/\"\n",
    "train_path = dataset_path + \"Train/\"\n",
    "test_path = dataset_path + \"Test/\"\n",
    "\n",
    "train_parquet_path = dataset_path + \"Parquets/Train/\"\n",
    "train_files = os.listdir(train_path) ### Change path to test directory when processing test files\n",
    "i = 0\n",
    "while(i<4):\n",
    "    df = pd.DataFrame(columns=[\"filename\", \"tif\", \"tfw\"])\n",
    "    files = train_files[90000*i:90000*(i+1)]\n",
    "    for instance in tqdm(files):\n",
    "        if \".tif\" in instance:\n",
    "            #### change the path when processing test file\n",
    "            im = Image.open(train_path + instance)\n",
    "        \n",
    "            image_bytes = io.BytesIO()\n",
    "            im.save(image_bytes, format=\"JPEG\")\n",
    "            image_bytes.seek(0)\n",
    "\n",
    "            #### change the path when processing test file\n",
    "            tfw_file_path = train_path + instance[:-4] + \".tfw\"\n",
    "            with open(tfw_file_path, 'rb') as tfw_file:\n",
    "                tfw_bytes = tfw_file.read()\n",
    "\n",
    "            new_data = {\"filename\": instance[:-4], \"tif\": image_bytes.getvalue(), \"tfw\":tfw_bytes}\n",
    "            new_data_df = pd.DataFrame([new_data])\n",
    "            df = pd.concat([df, new_data_df], ignore_index=True)\n",
    "    #### change the location to dump the test parquet file\n",
    "    df.to_parquet(train_parquet_path + 'train' + str(i) + '.parquet')\n",
    "    i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code to check the the parquet file is being processed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000\n",
      "0.0000000000\n",
      "0.0000000000\n",
      "1.0000000000\n",
      "3072.0000000000\n",
      "2304.0000000000\n",
      "\n",
      "['1.0000000000', '0.0000000000', '0.0000000000', '1.0000000000', '3072.0000000000', '2304.0000000000']\n"
     ]
    }
   ],
   "source": [
    "#### change the path to read parquet file\n",
    "df = pd.read_parquet('/Users/sriharshagaddipati/Documents/SG_Downloads/ex.parquet')\n",
    "for index, row in df.iterrows():\n",
    "    image_data = row['tif']\n",
    "    im = Image.open(io.BytesIO(image_data))\n",
    "    print(im)\n",
    "    im_arr = np.asarray(im)\n",
    "    print(im_arr.shape)\n",
    "    plt.imshow(im_arr)\n",
    "    plt.show()\n",
    "\n",
    "    tfw_bytes = row['tfw']\n",
    "    tfw = tfw_bytes.decode('utf-8')\n",
    "    print(tfw)\n",
    "\n",
    "    t = tfw.split()\n",
    "    print(t)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Place the hugging face token in the below command to write to datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/sriharshagaddipati/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token [token_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 225/225 [00:00<00:00, 695.59ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 225/225 [00:00<00:00, 797.16ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 2/2 [03:45<00:00, 112.82s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 225/225 [00:00<00:00, 609.95ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 225/225 [00:00<00:00, 677.94ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 2/2 [04:15<00:00, 127.53s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 225/225 [00:00<00:00, 601.35ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 225/225 [00:00<00:00, 638.14ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 2/2 [04:02<00:00, 121.29s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 225/225 [00:00<00:00, 656.78ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 225/225 [00:00<00:00, 884.87ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 2/2 [03:53<00:00, 116.83s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    df = pd.read_parquet(train_parquet_path + 'train' + str(i) + '.parquet')\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    dataset.push_to_hub(\"back2classroom/sidewalks_chunk\" + str(i+1), split=\"train\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pushing Test dataset to val split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 200/200 [00:00<00:00, 765.56ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [01:48<00:00, 108.81s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/back2classroom/sidewalks/commit/90469dc8766aeeed96d45771036d09f2e958eecf', commit_message='Upload dataset', commit_description='', oid='90469dc8766aeeed96d45771036d09f2e958eecf', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## change the path to location of test parquet file\n",
    "df = pd.read_parquet('/Volumes/T7 Shield/Downloads/Parquets/Test/test.parquet')\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset.push_to_hub(\"back2classroom/sidewalks\", split=\"val\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pushing Train dataset to train split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 298M/298M [00:16<00:00, 18.1MB/s] \n",
      "Downloading data: 100%|██████████| 298M/298M [00:17<00:00, 17.0MB/s] \n",
      "Generating train split: 100%|██████████| 45000/45000 [00:00<00:00, 62738.85 examples/s]\n",
      "Downloading readme: 100%|██████████| 349/349 [00:00<00:00, 510kB/s]\n",
      "Downloading data: 100%|██████████| 328M/328M [00:19<00:00, 16.6MB/s] \n",
      "Downloading data: 100%|██████████| 328M/328M [00:21<00:00, 15.0MB/s] \n",
      "Generating train split: 100%|██████████| 45000/45000 [00:00<00:00, 67004.87 examples/s]\n",
      "Downloading readme: 100%|██████████| 349/349 [00:00<00:00, 1.90MB/s]\n",
      "Downloading data: 100%|██████████| 320M/320M [00:18<00:00, 17.3MB/s] \n",
      "Downloading data: 100%|██████████| 314M/314M [00:22<00:00, 14.2MB/s] \n",
      "Generating train split: 100%|██████████| 45000/45000 [00:00<00:00, 71628.69 examples/s]\n",
      "Downloading readme: 100%|██████████| 349/349 [00:00<00:00, 1.33MB/s]\n",
      "Downloading data: 100%|██████████| 306M/306M [00:19<00:00, 15.9MB/s] \n",
      "Downloading data: 100%|██████████| 308M/308M [00:16<00:00, 18.9MB/s] \n",
      "Generating train split: 100%|██████████| 45000/45000 [00:00<00:00, 54183.77 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 300/300 [00:01<00:00, 251.53ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 300/300 [00:01<00:00, 283.69ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 300/300 [00:01<00:00, 265.53ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 300/300 [00:01<00:00, 265.64ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 300/300 [00:01<00:00, 268.60ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 300/300 [00:01<00:00, 285.98ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 6/6 [14:55<00:00, 149.22s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/back2classroom/sidewalks/commit/b8047b0aeecb8145bc04b5dadc8a4aad0f6b01ef', commit_message='Upload dataset', commit_description='', oid='b8047b0aeecb8145bc04b5dadc8a4aad0f6b01ef', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_chunks = []\n",
    "\n",
    "dataset_chunk = load_dataset(\"back2classroom/sidewalks_chunk1\")\n",
    "dataset_chunks.append(dataset_chunk['train'])\n",
    "\n",
    "dataset_chunk = load_dataset(\"back2classroom/sidewalks_chunk2\")\n",
    "dataset_chunks.append(dataset_chunk['train'])\n",
    "\n",
    "dataset_chunk = load_dataset(\"back2classroom/sidewalks_chunk3\")\n",
    "dataset_chunks.append(dataset_chunk['train'])\n",
    "\n",
    "dataset_chunk = load_dataset(\"back2classroom/sidewalks_chunk4\")\n",
    "dataset_chunks.append(dataset_chunk['train'])\n",
    "\n",
    "# Concatenate datasets\n",
    "concatenated_dataset = concatenate_datasets(dataset_chunks)\n",
    "\n",
    "# Push the concatenated dataset to the Hub\n",
    "concatenated_dataset.push_to_hub(\"back2classroom/sidewalks\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
